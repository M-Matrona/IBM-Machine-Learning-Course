{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b63159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict,cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec97de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "dataset = 'bodyfat.csv'\n",
    "path = os.getcwd() + '\\\\' + dataset\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "Age = df['Age']\n",
    "#df.drop(columns=['Age'], inplace=True) ##\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e4d30",
   "metadata": {},
   "source": [
    "#https://www.kaggle.com/fedesoriano/body-fat-prediction-dataset\n",
    "\n",
    "The percentage of body fat for an individual can be estimated once body density has been determined. Folks (e.g. Siri (1956)) assume that the body consists\n",
    "of two components - lean body tissue and fat tissue. Letting:\n",
    "\n",
    "D = Body Density (gm/cm^3) - This is measured - hydrostatic weighing\n",
    "\n",
    "\n",
    "A = proportion of lean body tissue  <br>\n",
    "B = proportion of fat tissue (A+B=1)\n",
    "\n",
    "\n",
    "a = density of lean body tissue (gm/cm^3) <br>\n",
    "b = density of fat tissue (gm/cm^3) - fat floats in water so (Wa - Ww) = lean body mass\n",
    "\n",
    "a and b have average value:\n",
    "\n",
    "Using the estimates \n",
    "\n",
    "a=1.10 gm/cm^3 and\n",
    "b=0.90 gm/cm^3 \n",
    "(see Katch and McArdle (1977), p. 111 or Wilmore (1976), p. 123) we come up with \"Siri's equation\":\n",
    "\n",
    "\n",
    "we have:\n",
    "\n",
    "D = 1/[(A/a) + (B/b)]\n",
    "\n",
    "solving for B we find:\n",
    "\n",
    "B = (1/D)*[ab/(a-b)] - [b/(a-b)].\n",
    "\n",
    "Percentage of Body Fat (i.e. 100*B)\n",
    "\n",
    "100*B = 495/D - 450\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47796220",
   "metadata": {},
   "source": [
    "Density from hydrostatic weighing.\n",
    "\n",
    "Body Density;\n",
    "\n",
    "D = WA/[(WA-WW)/c.f. - LV]\n",
    "\n",
    "WA = Weight in air (kg) <br>\n",
    "WW = Weight in water (kg)\n",
    "\n",
    "c.f. = Water correction factor (=1 at 39.2 deg F as one-gram of water occupies exactly one cm^3 at this temperature, =.997 at 76-78 deg F)\n",
    "\n",
    "c.f. is really the density of water at test temperature\n",
    "\n",
    "(WA-WW)/c.f. is the volume of water displaced by submerged body\n",
    "Archemedies Principle\n",
    "\n",
    "LV = Residual Lung Volume (liters)\n",
    "\n",
    "Have to assume LV was accounted for appropiately\n",
    "\n",
    "B = (1/D)*[ab/(a-b)] - [b/(a-b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BodyFatP(D, a = 1.1, b = .9):\n",
    "    \"\"\"returns percentage of body fat as estimated by siri equation\"\"\"\n",
    "    B = 100*((1/D)*(a*b)/(a-b) - b/(a-b))\n",
    "    return B\n",
    "\n",
    "def calc_BMI(H, W):\n",
    "    \"\"\"Calculate BMI from Height (inches) and Weight (lbs)\"\"\"\n",
    "    BMI = 703*W/H**2\n",
    "    return BMI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c30ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_d = min(df['Density'])\n",
    "max_d = max(df['Density'])\n",
    "\n",
    "print('Minimum density in dataset = ', str(min_d))\n",
    "print('the lower bound seems reasonable wrt the Siri equation Coefficient\\n')\n",
    "\n",
    "print('Maximum density in dataset = ', str(max_d))\n",
    "print('however the upper bound is suspicious because an entire body density is about the average of lean tissue\\n')\n",
    "\n",
    "bodyfat_max_d = BodyFatP(max_d)\n",
    "\n",
    "print('body fat percentage corresponding to max density in dataset = ', str(bodyfat_max_d))\n",
    "print('which is clearly nonsensical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b070941",
   "metadata": {},
   "outputs": [],
   "source": [
    "DensityBounds = np.linspace(min(df['Density']), max(df['Density']), 25)\n",
    "SiriBP = [BodyFatP(i) for i in DensityBounds]\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.scatter(df['Density'],df['BodyFat'])\n",
    "ax.plot(DensityBounds,SiriBP,'r')\n",
    "\n",
    "ax.set_xlabel('Density (g/cm**3)')\n",
    "ax.set_ylabel('Body Fat %')\n",
    "ax.legend(['Siri Equation', 'Reported Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the data that is off of the siri equation line\n",
    "DensityPoints = df['Density']\n",
    "BodyFatP_Points = df['BodyFat']\n",
    "\n",
    "SiriBP_D = [BodyFatP(i) for i in DensityPoints]\n",
    "\n",
    "df_Sc = df[['Density', 'BodyFat']]\n",
    "df_Sc.insert(loc=2, column='BodyFatCalc',value=SiriBP_D)\n",
    "series1 = df_Sc['BodyFat'] - df_Sc['BodyFatCalc']\n",
    "df_Sc = df_Sc[abs(series1)>0.5]\n",
    "\n",
    "\n",
    "print(df_Sc.shape)\n",
    "df_Sc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean has removed the 5 points found above\n",
    "\n",
    "df_clean = df[abs(series1)<.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visual confirmation that data was removed successfully')\n",
    "DensityBounds = np.linspace(min(df_clean['Density']), max(df_clean['Density']), 25)\n",
    "SiriBP = [BodyFatP(i) for i in DensityBounds]\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.scatter(df_clean['Density'],df_clean['BodyFat'])\n",
    "ax.plot(DensityBounds,SiriBP,'r')\n",
    "\n",
    "ax.set_xlabel('Density (g/cm**3)')\n",
    "ax.set_ylabel('Body Fat %')\n",
    "ax.legend(['Siri Equation', 'Reported Data'])\n",
    "fig.suptitle('Suspect Data Removed', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c23ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting a BMI Feature\n",
    "df_clean.insert(loc=2, column='BMI',value=calc_BMI(df_clean['Height'],df_clean['Weight']))\n",
    "df_clean.sort_values(by='BMI', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e54499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Using the BMI feature to count Obesity in the data and for outlier checks\n",
    "# print('min BMI in data ' + str(min(df_clean['BMI'])))\n",
    "# print('max BMI in data ' + str(max(df_clean['BMI'])))\n",
    "\n",
    "df_cleaner = df_clean[df_clean['BMI']<100] #remove the outlying data\n",
    "\n",
    "# num_of_obese = len(df_cleaner[df_cleaner['BMI']>30])\n",
    "\n",
    "# print('\\n')\n",
    "# print('the number of obese men in this dataset is ' + str(num_of_obese))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09af79",
   "metadata": {},
   "source": [
    "df_cleaner is the master dataframe in which all obviously outlying datapoints are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next will be to construct a pairplot to examine feature association\n",
    "#sns.pairplot(df_cleaner[df_cleaner['BMI']<25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skew(df):\n",
    "    \"\"\"function to check the normality of the float columns of a dataframe.\n",
    "        Uses D'Agostino's K-Squared test\"\"\"\n",
    "    skewlist = []\n",
    "    vallist = []\n",
    "    X_cols = df.select_dtypes(include = 'float').columns.to_list()\n",
    "\n",
    "    for col in X_cols:\n",
    "        ob = normaltest(df[col])\n",
    "        if ob.pvalue < 0.05:\n",
    "            skewlist.append(col)\n",
    "            vallist.append(min(df[col])) #keep track of min value to ensure we are only working with nonnegative data\n",
    "    return skewlist, vallist       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f6c10",
   "metadata": {},
   "source": [
    "### Begin Unsupervised Learning Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import normaltest # D'Agostino K^2 Test\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77003281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef = pd.DataFrame() #initialize dataframe to store R2 coefficients for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have previously done some work justifying the two outlying ankly data points.  The pairplot can be used to visually confirm.\n",
    "df_cleaner = df_cleaner[df_cleaner['Ankle']<28]\n",
    "df = df_cleaner.copy() #df used to check skews and normalize \n",
    "\n",
    "\n",
    "df_bmi=df_cleaner.copy() #keep this copy before all the transformations so that \n",
    "#list of groups can be appended to a human interpretable version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954bcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df_cleaner.drop(columns=['BodyFat', 'Density'])\n",
    "\n",
    "Y_data = df_cleaner['BodyFat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=72018)\n",
    "\n",
    "s = StandardScaler()\n",
    "\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_test_s = s.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6855478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso Regression')\n",
    "\n",
    "#Use the lassoCV function to come up with alpha\n",
    "\n",
    "\n",
    "lassoCV = LassoCV(alphas=np.linspace(.0005,.00065,20),\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train_s, y_train)\n",
    "\n",
    "alpha = lassoCV.alpha_\n",
    "print('alpha = ', alpha)\n",
    "\n",
    "\n",
    "y_pred_lasso = lassoCV.predict(X_test_s)\n",
    "r2=r2_score(y_test,y_pred_lasso)\n",
    "\n",
    "try:\n",
    "    df_coef['NO_Cluster'] =[r2]\n",
    "except:\n",
    "    pass \n",
    "\n",
    "print('r2 =', r2)\n",
    "\n",
    "print(abs(lassoCV.coef_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell was a normalizing exercise that proved unnecessary at this stage when the unsupervised learning algorithms were being explored.\n",
    "\n",
    "\n",
    "skewlist, vallist = check_skew(df)\n",
    "\n",
    "print('columns with skew = ', skewlist)   \n",
    "\n",
    "\n",
    "assert len([i for i in vallist if i < 0]) == 0 #make sure log transform is valid\n",
    "\n",
    "df_norm = df.copy()\n",
    "for x in skewlist:\n",
    "    df_norm[x] = np.log(df[x]) \n",
    "    \n",
    "c1, c2 = check_skew(df_norm)\n",
    "\n",
    "print('skewed columns left after a log transform =', c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6cd8e",
   "metadata": {},
   "source": [
    "#### The goal of the unsupervised models is to see if they can come up with the obese/ not obese split that is in line with the overweight definition at BMI = 25.   \n",
    "\n",
    "#### In the supervised learning lab I used this fact to bin my data for the sake of normality.  It will be interesting to see if the algorithms come up with better splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff491381",
   "metadata": {},
   "source": [
    "#### Through trial and error I learned that not transforming  the skewed columns leads to outlier detection that is more in line with human judgement.  6 more overweight/obese men are classified as outliers when the DMDBSCAN alogithm is run without the data transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b42a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bmi.skew().apply(abs).sort_values(ascending = False)\n",
    "df_final = df_bmi.copy()\n",
    "df = df_bmi.copy().drop(columns='BMI') #unnormalized data with the BMI column dropped -\n",
    "#                                       I don't want to give any hints to the algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#the combination of these two will reintroduce the DMDBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4f56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xu = df.drop(columns=['Density','BodyFat']) #these are related to targets\n",
    "\n",
    "s, l = check_skew(Xu)\n",
    "\n",
    "for i in s:\n",
    "    Xu[i] = np.log(Xu[i])\n",
    "\n",
    "y = df['BodyFat']\n",
    "\n",
    "s = StandardScaler() #normalizing is important for these distance based algorithms\n",
    "X = s.fit_transform(Xu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this code is used to find the optimal epsilon\n",
    "neigh = NearestNeighbors(n_neighbors=3)\n",
    "nbrs = neigh.fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca23066",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = np.sort(distances, axis=0) #sorts each points distance to neighbor\n",
    "distances = distances[:,1] #keep the first nonzero entry of each row array\n",
    "plt.plot(distances)\n",
    "\n",
    "plt.xlabel('point number')\n",
    "plt.ylabel('distance to nearest neighbor')\n",
    "plt.title('DMDBSCAN - epsilon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6550a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=[]\n",
    "ep=2.25\n",
    "for i in range(1,25):\n",
    "    dbs = DBSCAN(eps=ep,min_samples=i, metric='euclidean')\n",
    "    dbs.fit(X)\n",
    "    outliers.append(np.count_nonzero(dbs.labels_==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd928ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(outliers,'o')\n",
    "plt.xlabel('min number of core samples for core point')\n",
    "plt.ylabel('number of outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the DBSCAN model with the optimized epsilon and min_samples\n",
    "\n",
    "dbs = DBSCAN(eps=ep,min_samples=3, metric='euclidean')\n",
    "dbs.fit(X)\n",
    "print(np.count_nonzero(dbs.labels_==-1))\n",
    "print(np.count_nonzero(dbs.labels_!=-1))\n",
    "print(np.unique(dbs.labels_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b24d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the cluster labels into the original dataset.  This is so we have a shot at identifying what makes an outlier\n",
    "df_final['Cluster_Assign_DB'] = dbs.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf561e46",
   "metadata": {},
   "source": [
    "#### The following cells are just to poke at the clustered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989fd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df_cluster.drop(['Density','BodyFat'],axis=1))\n",
    "\n",
    "#![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22667b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df_bmi.copy()\n",
    "df_cluster['Cluster_Assign_DB'] = dbs.labels_\n",
    "df_cluster=df_cluster[df_cluster['Cluster_Assign_DB']==0]\n",
    "df_cluster.drop(['Cluster_Assign_DB'],axis=1,inplace=True)\n",
    "\n",
    "X_data = df_cluster.drop(columns=['BodyFat', 'Density'])\n",
    "Y_data = df_cluster['BodyFat']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=72018)\n",
    "\n",
    "\n",
    "\n",
    "s=StandardScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "\n",
    "print('Lasso Regression')\n",
    "\n",
    "#Use the lassoCV function to come up with alpha\n",
    "\n",
    "\n",
    "lassoCV = LassoCV(alphas=np.linspace(1e-6,.1e-4,200),\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train_s, y_train)\n",
    "\n",
    "alpha = lassoCV.alpha_\n",
    "print('alpha = ', alpha)\n",
    "\n",
    "\n",
    "y_pred_lasso = lassoCV.predict(X_test_s)\n",
    "r2=r2_score(y_test,y_pred_lasso)\n",
    "try: \n",
    "    df_coef['DBSCAN'] =[r2]\n",
    "except:\n",
    "    pass\n",
    "print('r2 =', r2)\n",
    "\n",
    "print(abs(lassoCV.coef_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75408098",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasc = lassoCV.coef_\n",
    "cols = X_data.columns.to_list()\n",
    "noPFdf = pd.DataFrame(lasc,cols)\n",
    "\n",
    "noPFdf[0].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd53f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred_lasso,y_test,'o',[0,35],[0,35])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual - (Measured)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fddfe1",
   "metadata": {},
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06536c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d64654",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xu = df.drop(columns=['Density','BodyFat']) #these are related to targets\n",
    "skewlist,_= check_skew(Xu)\n",
    "\n",
    "for i in skewlist:\n",
    "    Xu[i]=np.log(Xu[i])\n",
    "\n",
    "y = df['BodyFat']\n",
    "\n",
    "s = StandardScaler()\n",
    "X = s.fit_transform(Xu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae985b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = KMeans(n_clusters=2)\n",
    "\n",
    "k2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inertia = []\n",
    "list_num_clusters = list(range(1,11))\n",
    "for num_clusters in list_num_clusters:\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(X)\n",
    "    inertia.append(km.inertia_)\n",
    "    \n",
    "plt.plot(list_num_clusters,inertia)\n",
    "plt.scatter(list_num_clusters,inertia)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2412a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Cluster_Assign_kmeans'] = k2.labels_\n",
    "\n",
    "df_bmi.columns\n",
    "df_bmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f47506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training regression model on kmeans cluster\n",
    "\n",
    "df_kmeans=df_bmi.copy()\n",
    "df_kmeans['Cluster_Assign_kmeans'] = k2.labels_\n",
    "\n",
    "df_kr = df_kmeans[df_kmeans['Cluster_Assign_kmeans']==1]\n",
    "\n",
    "df_cluster = df_kr.drop(columns = ['Cluster_Assign_kmeans'])\n",
    "\n",
    "X_data = df_cluster.drop(columns=['BodyFat', 'Density'])\n",
    "Y_data = df_cluster['BodyFat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=72018)\n",
    "\n",
    "s = StandardScaler()\n",
    "\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "\n",
    "print('Lasso Regression')\n",
    "\n",
    "#Use the lassoCV function to come up with alpha\n",
    "\n",
    "\n",
    "lassoCV = LassoCV(alphas=np.linspace(.02,.2,20),\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train_s, y_train)\n",
    "\n",
    "alpha = lassoCV.alpha_\n",
    "print('alpha = ', alpha)\n",
    "\n",
    "y_pred_lasso = lassoCV.predict(X_test_s)\n",
    "\n",
    "r2=r2_score(y_test,y_pred_lasso)\n",
    "try: \n",
    "    df_coef['kmeans'] = [r2]\n",
    "except:    \n",
    "    pass\n",
    "\n",
    "print('r2 =', r2_score(y_test,y_pred_lasso))\n",
    "\n",
    "print(abs(lassoCV.coef_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc0536",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#training regression model on kmeans cluster\n",
    "\n",
    "df_kmeans=df_bmi.copy()\n",
    "df_kmeans['Cluster_Assign_kmeans'] = k2.labels_\n",
    "\n",
    "df_kr = df_kmeans[df_kmeans['Cluster_Assign_kmeans']==0]\n",
    "\n",
    "df_cluster = df_kr.drop(columns = ['Cluster_Assign_kmeans'])\n",
    "\n",
    "X_data = df_cluster.drop(columns=['BodyFat', 'Density'])\n",
    "Y_data = df_cluster['BodyFat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=72018)\n",
    "\n",
    "s = StandardScaler()\n",
    "\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "\n",
    "print('Lasso Regression')\n",
    "\n",
    "#Use the lassoCV function to come up with alpha\n",
    "\n",
    "\n",
    "lassoCV = LassoCV(alphas=np.linspace(.2,.5,20),\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train_s, y_train)\n",
    "\n",
    "alpha = lassoCV.alpha_\n",
    "print('alpha = ', alpha)\n",
    "\n",
    "y_pred_lasso = lassoCV.predict(X_test_s)\n",
    "\n",
    "r2=r2_score(y_test,y_pred_lasso)\n",
    "try: \n",
    "    df_coef['kmeans'] = [r2]\n",
    "except:    \n",
    "    pass\n",
    "\n",
    "print('r2 =', r2_score(y_test,y_pred_lasso))\n",
    "\n",
    "print(abs(lassoCV.coef_).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasc = lassoCV.coef_\n",
    "cols = X_data.columns.to_list()\n",
    "noPFdf = pd.DataFrame(lasc,cols)\n",
    "\n",
    "noPFdf[0].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ef82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred_lasso,y_test,'o',[0,35],[0,35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339a9bd",
   "metadata": {},
   "source": [
    "## HA Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08012abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7120a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xu = df.drop(columns=['Density','BodyFat']) #these are related to targets\n",
    "y = df['BodyFat']\n",
    "\n",
    "skewlist,_= check_skew(Xu)\n",
    "\n",
    "for i in skewlist:\n",
    "    Xu[i]=np.log(Xu[i])\n",
    "\n",
    "\n",
    "\n",
    "s = StandardScaler() #normalizing is important for these distance based algorithms\n",
    "X = s.fit_transform(Xu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "HA = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HA.fit(Xu) #training on unstandardized data leads to drastically better model performance - why\n",
    "HA.labels_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b04d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Cluster_Assign_HA'] = HA.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training regression model on HA cluster\n",
    "\n",
    "df_ha = df_bmi.copy()\n",
    "df_ha['Cluster_Assign_HA'] = HA.labels_\n",
    "df_ha_r = df_ha[df_ha['Cluster_Assign_HA']==0]\n",
    "\n",
    "df_cluster = df_ha_r.drop(columns=['Cluster_Assign_HA'])\n",
    "\n",
    "X_data = df_cluster.drop(columns=['BodyFat', 'Density'])\n",
    "Y_data = df_cluster['BodyFat']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=72018)\n",
    "\n",
    "s = StandardScaler()\n",
    "\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "\n",
    "print('Lasso Regression')\n",
    "\n",
    "#Use the lassoCV function to come up with alpha\n",
    "\n",
    "\n",
    "lassoCV = LassoCV(alphas=np.linspace(.15,.25,20),\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train_s, y_train)\n",
    "\n",
    "alpha = lassoCV.alpha_\n",
    "print('alpha = ', alpha)\n",
    "\n",
    "y_pred_lasso = lassoCV.predict(X_test_s)\n",
    "\n",
    "r2=r2_score(y_test,y_pred_lasso)\n",
    "df_coef['HA'] = r2\n",
    "\n",
    "\n",
    "print('r2 =', r2_score(y_test,y_pred_lasso))\n",
    "\n",
    "print(abs(lassoCV.coef_).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef4c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasc = lassoCV.coef_\n",
    "cols = X_data.columns.to_list()\n",
    "noPFdf = pd.DataFrame(lasc,cols)\n",
    "\n",
    "noPFdf[0].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef449d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred_lasso,y_test,'o',[0,35],[0,35])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual - (Measured)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5223bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d93e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc415124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df_cluster.drop(['Density','BodyFat'],axis=1))\n",
    "try:\n",
    "    df_coef['HA'] = [r2]\n",
    "except:\n",
    "    df_coef.loc['HA'] = [r2]\n",
    "\n",
    "df_coef = df_coef.T\n",
    "df_coef.rename(columns={0:'R2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60869af4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
